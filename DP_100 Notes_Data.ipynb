{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notebook mainly focuses on terminoloy on Azure, methods available and process\n",
    "There is no machine learning calculation or model, any modeling will be denoted as model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Data</h3>\n",
    "Every workspace has two built-in datastores (an Azure Storage blob container, and an Azure Storage file container) that are used as system storage by Azure Machine Learning. There's also a third datastore that gets added to your workspace if you make use of the open datasets\n",
    "\n",
    "\n",
    "<ul>\n",
    "    <li>Azure Storage (blobs and file containers)</li>\n",
    "    <li>Data Lake</li>\n",
    "    <li>SQL Db</li>\n",
    "    <li>Databricks file system(DBFS</li>\n",
    "</ul>\n",
    "\n",
    "Two ways to add to datastore, GUI or SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Datastore\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# Register a new datastore\n",
    "blob_ds = Datastore.register_azure_blob_container(workspace=ws, \n",
    "                                                  datastore_name='blob_data', \n",
    "                                                  container_name='data_container',\n",
    "                                                  account_name='az_store_acct',\n",
    "                                                  account_key='123456abcde789â€¦')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get a reference to any datastore by using the Datastore.get() method as shown here:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_store = Datastore.get(ws, datastore_name='blob_data')\n",
    "\n",
    "# ws always include a default datastore\n",
    "default_store = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have a reference to a Workspace named ws. Which code retrieves the default datastore for the workspace? ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Considerations for datastores\n",
    "When planning for datastores, consider the following guidelines:\n",
    "<br>\n",
    "<ul>\n",
    "    <li>When using Azure blob storage, premium level storage may provide improved I/O performance for large datasets. However, this option will increase cost and may limit replication options for data redundancy.</li>\n",
    "    <li>When working with data files, although CSV format is very common, <b>Parquet</b> format generally results in better performance.</li>\n",
    "<li>You can access any datastore by name, but you may want to consider changing the default datastore (which is initially the built-in workspaceblobstore datastore).<br>\n",
    "    <code>ws.set_default_datastore('blob_data')</code>\n",
    "    </li>\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Types\n",
    "<ul>\n",
    "    <li>Tablular: data read as table. You should use this type of dataset when your data is <b>consistently structured</b> and you want to work with it in common tabular data structures,\n",
    "    <p>You can convert the tabular to pandas:\n",
    "        <code>df = tab_ds.to_pandas_dataframe()</code>\n",
    "    </li>\n",
    "    <li>File: The dataset presents a list of file paths that can be read as though from the file system. Use this type of dataset when your data is <b>unstructured</b>, or when you need to process the data at the file level </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to model registration, you can do the same with data. Register the data and retrive the data. \n",
    "\n",
    "<code>file_ds = Dataset.File.from_files(path=img_paths)\n",
    "file_ds = file_ds.register(workspace=ws, name='img_files', create_new_version=True)</code>\n",
    "\n",
    "And to retrive \n",
    "<code>img_ds = Dataset.get_by_name(workspace=ws, name='img_files', version=2)</code>\n",
    "\n",
    "Can also do the same for passing arguments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scipt argument for a file dataset\n",
    "Unlike with a tabular dataset, you must specify a mode for the file dataset argument, which can be as_download or as_mount. This provides an access point that the script can use to read the files in the dataset. In most cases, you should use as_download, which copies the files to a temporary location on the compute where the script is being run. However, if you are working with a large amount of data for which there may not be enough storage space on the experiment compute, use as_mount to stream\n",
    "\n",
    "- stream: as_mount\n",
    "- copies to temp location: as_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
