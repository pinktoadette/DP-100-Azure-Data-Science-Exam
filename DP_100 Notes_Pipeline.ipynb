{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n",
    "Pipeline: create a workflow to implements steps; steps can be sequentially or parallel. Each step can be run on a specific compute target. \n",
    "Common Azure ML pipeline steps:\n",
    "<ul>\n",
    "<li>PythonScript </li>\n",
    "<li>DataTransfer: Data Factory to copy data between data stores</li>\n",
    "<li>DataBricks: Runs a notebook, scipt</li>\n",
    "<li>AdlaStep: Runs a SQL job in Data Lake</li>\n",
    "<li>ParallelRun: runs python script on multiple compute nodes</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "\n",
    "# Step to run a Python script\n",
    "step1 = PythonScriptStep(name = 'prepare data',\n",
    "                         source_directory = 'scripts',\n",
    "                         script_name = 'data_prep.py',\n",
    "                         compute_target = 'aml-cluster')\n",
    "\n",
    "# Step to train a model\n",
    "step2 = PythonScriptStep(name = 'train model',\n",
    "                         source_directory = 'scripts',\n",
    "                         script_name = 'train_model.py',\n",
    "                         compute_target = 'aml-cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the steps, you can assign them to a pipeline, and run it as an experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import Pipeline\n",
    "from azureml.core import Experiment\n",
    "\n",
    "# Construct the pipeline\n",
    "train_pipeline = Pipeline(workspace = ws, steps = [step1,step2])\n",
    "\n",
    "# Create an experiment and run the pipeline\n",
    "experiment = Experiment(workspace = ws, name = 'training-pipeline')\n",
    "pipeline_run = experiment.submit(train_pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline Data Object\n",
    "- References a location in datastore\n",
    "- Creates a data dependency between pipeline steps\n",
    "<p>\n",
    "To use a PipelineData object to pass data between steps, you must:\n",
    "<ol>\n",
    "    <li>Define a PipelineData obj that references the location</li>\n",
    "    <li>Pass PipelineData obj as a script in argument in steps that run scripts</li>\n",
    "    <li>Specify if it is input or output</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep, EstimatorStep\n",
    "\n",
    "# Get a dataset for the initial data\n",
    "raw_ds = Dataset.get_by_name(ws, 'raw_dataset')\n",
    "\n",
    "##1() Define a PipelineData object to pass data between steps\n",
    "data_store = ws.get_default_datastore()\n",
    "prepped_data = PipelineData('prepped',  datastore=data_store)\n",
    "\n",
    "# Step to run a Python script\n",
    "step1 = PythonScriptStep(name = 'prepare data',\n",
    "                         source_directory = 'scripts',\n",
    "                         script_name = 'data_prep.py',\n",
    "                         compute_target = 'aml-cluster',\n",
    "                         ##2) Script arguments include PipelineData\n",
    "                         arguments = ['--raw-ds', raw_ds.as_named_input('raw_data'),\n",
    "                                      '--out_folder', prepped_data],\n",
    "                         ##3) Specify PipelineData as output\n",
    "                         outputs=[prepped_data])\n",
    "\n",
    "# Step to run an estimator\n",
    "step2 = PythonScriptStep(name = 'train model',\n",
    "                         source_directory = 'scripts',\n",
    "                         script_name = 'data_prep.py',\n",
    "                         compute_target = 'aml-cluster',\n",
    "                         # Pass as script argument\n",
    "                         arguments=['--in_folder', prepped_data],\n",
    "                         # Specify PipelineData as input\n",
    "                         inputs=[prepped_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reuse pipeline steps\n",
    "Azure ML includes caching and reuse features to reduce time. Time needs to be reduced because multiple steps can take a long time to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step1 = PythonScriptStep(name = 'prepare data',\n",
    "                         source_directory = 'scripts',\n",
    "                         script_name = 'data_prep.py',\n",
    "                         compute_target = 'aml-cluster',\n",
    "                         runconfig = run_config,\n",
    "                         inputs=[raw_ds.as_named_input('raw_data')],\n",
    "                         outputs=[prepped_data],\n",
    "                         arguments = ['--folder', prepped_data]),\n",
    "                         ##### Disable step reuse\n",
    "                         allow_reuse = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Forcing all steps to run\n",
    "`pipeline_run = experiment.submit(train_pipeline, regenerate_outputs=True)`\n",
    "\n",
    "Force them to rerun regardless of individual reuse configuration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Publish Pipeline\n",
    "Publish to REST endpoint, which the pipeline can be run on demand. There are two ways to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "published_pipeline = pipeline.publish(name='training_pipeline',\n",
    "                                          description='Model training pipeline',\n",
    "                                          version='1.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Publish method on a <b>successful</b> run of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most recent run of the pipeline\n",
    "pipeline_experiment = ws.experiments.get('training-pipeline')\n",
    "run = list(pipeline_experiment.get_runs())[0]\n",
    "\n",
    "# Publish the pipeline from the run\n",
    "published_pipeline = run.publish_pipeline(name='training_pipeline',\n",
    "                                          description='Model training pipeline',\n",
    "                                          version='1.0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After it is published, you can view it.<br>\n",
    "`rest_endpoint = published_pipeline.endpoint` <br>\n",
    "`print(rest_endpoint)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using a published pipeline\n",
    "It is just like calling an API call on your endpoint. Nothing special."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pipeline Parameters\n",
    "Pipeline can have parameters. Import `PipelineParameter` and specify each parameter at least one step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core.graph import PipelineParameter\n",
    "\n",
    "reg_param = PipelineParameter(name='reg_rate', default_value=0.01)\n",
    "\n",
    "\n",
    "step2 = PythonScriptStep(name = 'train model',\n",
    "                         source_directory = 'scripts',\n",
    "                         script_name = 'data_prep.py',\n",
    "                         compute_target = 'aml-cluster',\n",
    "                         # Pass parameter as script argument\n",
    "                         arguments=['--in_folder', prepped_data,\n",
    "                                    '--reg', reg_param],\n",
    "                         inputs=[prepped_data])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Schedule Pipeline\n",
    "Think of it as cronjobs, similar.<br>\n",
    "You can also schedule when there is data changes. You must create a Schedule that monitors a specified path on a datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Datastore\n",
    "from azureml.pipeline.core import Schedule\n",
    "\n",
    "training_datastore = Datastore(workspace=ws, name='blob_data')\n",
    "pipeline_schedule = Schedule.create(ws, name='Reactive Training',\n",
    "                                    description='trains model on data change',\n",
    "                                    pipeline_id=published_pipeline_id,\n",
    "                                    experiment_name='Training_Pipeline',\n",
    "                                    \n",
    "                                    #### SCHEDULES WHEN A DATA CHANGES\n",
    "                                    datastore=training_datastore,\n",
    "                                    path_on_datastore='data/training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowledge check:\n",
    "- You're creating a pipeline that includes two steps. Step 1 preprocesses some data, and step 2 uses the preprocessed data to train a model. What type of object should you use to pass data from step 1 to step 2 and create a dependency between these steps? Pipeline Data\n",
    "- You've published a pipeline that you want to run every week. You plan to use the Schedule.create method to create the schedule. What kind of object must you create first to configure how frequently the pipeline runs? ScheduleRecurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
